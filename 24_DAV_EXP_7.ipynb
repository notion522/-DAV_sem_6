{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment - 7: Perform the steps involved in Text Analytics in Python & R\n"
      ],
      "metadata": {
        "id": "w2nHLOesIyQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task to be performed :\n",
        "Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "\n",
        "Perform the following experiments using Python & R\n",
        "Tokenization (Sentence & Word)\n",
        "\n",
        "Frequency Distribution\n",
        "\n",
        "Remove stopwords & punctuations\n",
        "\n",
        "Lexicon Normalization (Stemming, Lemmatization)\n",
        "\n",
        "Part of Speech tagging\n",
        "\n",
        "Named Entity Recognization\n",
        "\n",
        "Scrape data from a website\n",
        "\n",
        "Prepare a document with the Aim, Tasks performed, Program, Output, and Conclusion."
      ],
      "metadata": {
        "id": "qrh9E1uZI5wF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python\n",
        "**NLTK (Natural Language Toolkit)**:\n",
        "\n",
        "NLTK is a powerful library for working with human language data and performing various tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, and named entity recognition.\n",
        "\n",
        "**spaCy:**\n",
        "\n",
        "spaCy is an open-source library for advanced natural language processing tasks. It provides pre-trained models for various languages and is known for its efficiency in terms of speed and memory usage.\n",
        "\n",
        "**TextBlob:**\n",
        "\n",
        "TextBlob is a simple and easy-to-use library for processing textual data. It offers a high-level interface for common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
        "\n",
        "**Gensim:**\n",
        "\n",
        "Gensim is a library for topic modeling and document similarity analysis. It is often used for unsupervised learning on large text corpora, providing implementations for algorithms like Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\n",
        "\n",
        "**BeautifulSoup (for Web Scraping):**\n",
        "\n",
        "BeautifulSoup is a library used for web scraping purposes. It helps to pull the data out of HTML and XML files and is widely used in conjunction with requests library to scrape and parse web content.\n",
        "Website: BeautifulSoup"
      ],
      "metadata": {
        "id": "XDdlvRKjJDDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization (Sentence & Word)"
      ],
      "metadata": {
        "id": "lHDHEx4sz7Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnHtWbslxH20",
        "outputId": "65a5ec10-381a-4ca0-f4a2-63d9d90aa202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-BAveNGwEQ1",
        "outputId": "686ad046-b618-4782-b70d-51cd7a9b1eb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.', 'Welcome to college']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#sent_tokenize is sentence tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to college\"\n",
        "sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing other languages\n",
        "\n",
        "import nltk.data\n",
        "\n",
        "spanish_tokenizer=nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "\n",
        "text = 'Hola amigo. Estoy bien.'\n",
        "spanish_tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KolujPatwy1F",
        "outputId": "303532d1-49f3-4395-9f71-15cb960dfe97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola amigo.', 'Estoy bien.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_tokenize is word tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hello everyone. Welcome to college\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4rhUG_uxzya",
        "outputId": "9f583af1-723b-446d-cc97-3ae0a4284395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'college']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution"
      ],
      "metadata": {
        "id": "1vaTa8JDz-K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "text = \"insights : Text analytics is the process of analyzing unstructured text data for useful insights and patterns.\"\n",
        "\n",
        "words=word_tokenize(text)\n",
        "\n",
        "fdist=FreqDist(words)\n",
        "\n",
        "print(\"Word\\tFrequency\")\n",
        "print(\"----------------\")\n",
        "for word,frequency in fdist.items():\n",
        "  print(f\"{word}\\t{frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpLFENQU0AIE",
        "outputId": "6fd28823-5f23-4508-8853-6ec276a26e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word\tFrequency\n",
            "----------------\n",
            "insights\t2\n",
            ":\t1\n",
            "Text\t1\n",
            "analytics\t1\n",
            "is\t1\n",
            "the\t1\n",
            "process\t1\n",
            "of\t1\n",
            "analyzing\t1\n",
            "unstructured\t1\n",
            "text\t1\n",
            "data\t1\n",
            "for\t1\n",
            "useful\t1\n",
            "and\t1\n",
            "patterns\t1\n",
            ".\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1cFGmi20Z75",
        "outputId": "d25a5b29-94cf-4b5b-b298-102c490e4505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_and_punctuation(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Get English stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Define punctuation characters\n",
        "    punctuations = set(string.punctuation)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    clean_tokens = [token for token in tokens if token.lower() not in stop_words and token not in punctuations]\n",
        "\n",
        "    # Reconstruct the text without stopwords and punctuation\n",
        "    clean_text = ' '.join(clean_tokens)\n",
        "\n",
        "    return clean_text\n",
        "cleaned_text = remove_stopwords_and_punctuation(text)\n",
        "print(\"Cleaned text:\", cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9NPeKzt1tNR",
        "outputId": "3f5ce42c-331d-4297-b4bb-69c5a781b08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text: insights Text analytics process analyzing unstructured text data useful insights patterns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Ln62H82apD",
        "outputId": "6fc7099d-6e6d-444a-d459-00d3947506d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Lemmatization takes into account the context of the word in a sentence and typically involves dictionary lookup and morphological analysis to determine the lemma. Unlike stemming, which simply removes prefixes or suffixes to produce a truncated form of the word (stem), lemmatization ensures that the resulting word is a valid word in the language's dictionary. This makes lemmatization more accurate but also computationally more intensive compared to stemming.`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bwi_oEwB4tKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens is the tokenized text\n",
        "tokens = word_tokenize(text)\n",
        "# Initialize Porter Stemmer and WordNet Lemmatizer\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(\"Original text:\", text)\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6rB5YbZ4BrI",
        "outputId": "c9985ae3-80cb-4a4d-d79f-167d8430402c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: insights : Text analytics is the process of analyzing unstructured text data for useful insights and patterns.\n",
            "Stemmed words: ['insight', ':', 'text', 'analyt', 'is', 'the', 'process', 'of', 'analyz', 'unstructur', 'text', 'data', 'for', 'use', 'insight', 'and', 'pattern', '.']\n",
            "Lemmatized words: ['insight', ':', 'Text', 'analytics', 'is', 'the', 'process', 'of', 'analyzing', 'unstructured', 'text', 'data', 'for', 'useful', 'insight', 'and', 'pattern', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here are some of the common Part of Speech (POS) tags used in the Penn Treebank POS tagging scheme:\n",
        "\n",
        "CC - Coordinating conjunction,\n",
        "CD - Cardinal number,\n",
        "DT - Determiner,\n",
        "EX - Existential there,\n",
        "FW - Foreign word,\n",
        "IN - Preposition or subordinating conjunction,\n",
        "JJ - Adjective,\n",
        "JJR - Adjective, comparative,\n",
        "JJS - Adjective, superlative,\n",
        "LS - List item marker,\n",
        "MD - Modal,\n",
        "NN - Noun, singular or mass,\n",
        "NNS - Noun, plural,\n",
        "NNP - Proper noun, singular,\n",
        "NNPS - Proper noun, plural,\n",
        "PDT - Predeterminer,\n",
        "POS - Possessive ending,\n",
        "PRP - Personal pronoun,\n",
        "PRP - Possessive pronoun,\n",
        "RB - Adverb,\n",
        "RBR - Adverb, comparative,\n",
        "RBS - Adverb, superlative,\n",
        "RP - Particle,\n",
        "SYM - Symbol,\n",
        "TO - to,\n",
        "UH - Interjection,\n",
        "VB - Verb, base form,\n",
        "VBD - Verb, past tense,\n",
        "VBG - Verb, gerund or present participle,\n",
        "VBN - Verb, past participle,\n",
        "VBP - Verb, non-3rd person singular present,\n",
        "VBZ - Verb, 3rd person singular present,\n",
        "WDT - Wh-determiner,\n",
        "WP - Wh-pronoun,\n",
        "WP$ - Possessive wh-pronoun,\n",
        "WRB - Wh-adverb\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jzJOf4yd5sOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')#needed for pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Vq-PIPU5YBj",
        "outputId": "93d7ec2a-6d79-4585-aef9-5507a2e93201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import ne_chunk#needed for named recognization\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paf1DVkx626c",
        "outputId": "50c77fa9-7217-402f-a71e-ba61f3633c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parts of speech tagging\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform Part of Speech tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Print the tagged words with their POS\n",
        "print(pos_tags)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp_v39lQ4LXK",
        "outputId": "c2da0e8f-d28b-4fd7-9387-9437863ffda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('insights', 'NNS'), (':', ':'), ('Text', 'NN'), ('analytics', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('analyzing', 'VBG'), ('unstructured', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('useful', 'JJ'), ('insights', 'NNS'), ('and', 'CC'), ('patterns', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "text = \"Barack Obama was born in Hawaii. He was the 44th President of the United States.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Tag the tokens with part-of-speech tags\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "# Perform named entity recognition\n",
        "named_entities = ne_chunk(tagged_tokens)\n",
        "\n",
        "# Print the named entities\n",
        "for entity in named_entities:\n",
        "    if isinstance(entity, nltk.Tree):\n",
        "        print(\" \".join([word for word, tag in entity]), \"-\", entity.label())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OrB6D915SUW",
        "outputId": "7a053139-0c2a-459c-c6ce-966c4f4a6c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Barack - PERSON\n",
            "Obama - PERSON\n",
            "Hawaii - GPE\n",
            "United States - GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#scaping data from a website\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the website you want to scrape\n",
        "url = 'https://stackoverflow.com/questions/4634787/freqdist-with-nltk'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find and extract the text data you're interested in\n",
        "    text_data = soup.get_text()\n",
        "\n",
        "    # Print or process the text data as needed\n",
        "    print(text_data)\n",
        "\n",
        "else:\n",
        "    print('Failed to retrieve data from the website')\n"
      ],
      "metadata": {
        "id": "hJF4LNYf7TBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab8962e-f9ff-4447-df73-6ef19523d2c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "python - FreqDist with NLTK - Stack Overflow\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stack Overflow\n",
            "\n",
            "\n",
            "\n",
            "About\n",
            "\n",
            "\n",
            "\n",
            "\t\t\t\t\t\tProducts\n",
            "\t\t\t\t\t\n",
            "\n",
            "\n",
            "For Teams\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stack Overflow\n",
            "Public questions & answers\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stack Overflow for Teams\n",
            "Where developers & technologists share private knowledge with coworkers\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Talent\n",
            "\n",
            "\t\t\t\t\t\t\t\tBuild your employer brand\n",
            "\t\t\t\t\t\t\t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Advertising\n",
            "Reach developers & technologists worldwide\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Labs\n",
            "The future of collective knowledge sharing\n",
            "\n",
            "\n",
            "\n",
            "About the company\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Loading…\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "current community\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            Stack Overflow\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "help\n",
            "chat\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            Meta Stack Overflow\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "your communities            \n",
            "\n",
            "\n",
            "\n",
            "Sign up or log in to customize your list.                \n",
            "\n",
            "\n",
            "more stack exchange communities\n",
            "\n",
            "company blog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Log in\n",
            "\n",
            "Sign up\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Home\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Questions\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Tags\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Users\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Companies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Labs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Discussions\n",
            "\n",
            "New\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Collectives\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Collectives™ on Stack Overflow\n",
            "        – Centralized & trusted content around the technologies you use the most.\n",
            "        \n",
            "        \n",
            "\n",
            " \n",
            "Learn more about Collectives\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Explore Collectives\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Teams\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                    Create free Team\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Collectives™ on Stack Overflow\n",
            "Find centralized, trusted content and collaborate around the technologies you use most.\n",
            "\n",
            "                    Learn more about Collectives\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Teams\n",
            "Q&A for work\n",
            "Connect and share knowledge within a single location that is structured and easy to search.\n",
            "\n",
            "                    Learn more about Teams\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Get early access and see previews of new features.\n",
            " Learn more about Labs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FreqDist with NLTK\n",
            "\n",
            "\n",
            "        Ask Question\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Asked\n",
            "13 years, 1 month ago\n",
            "\n",
            "\n",
            "Modified\n",
            "1 year, 3 months ago\n",
            "\n",
            "\n",
            "Viewed\n",
            "                        111k times\n",
            "                    \n",
            "\n",
            " Part of NLP Collective\n",
            "                        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            35\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Python package nltk has the FreqDist function which gives you the frequency of words within a text. I am trying to pass my text as an argument but the result is of the form:\n",
            "[' ', 'e', 'a', 'o', 'n', 'i', 't', 'r', 's', 'l', 'd', 'h', 'c', 'y', 'b', 'u', 'g', '\\n', 'm', 'p', 'w', 'f', ',', 'v', '.', \"'\", 'k', 'B', '\"', 'M', 'H', '9', 'C', '-', 'N', 'S', '1', 'A', 'G', 'P', 'T', 'W', '[', ']', '(', ')', '0', '7', 'E', 'J', 'O', 'R', 'j', 'x']\n",
            "\n",
            "whereas in the example on the nltk website, the result was whole words not characters. Here is how I am currently using the function:\n",
            "file_y = open(fileurl)\n",
            "p = file_y.read()\n",
            "fdist = FreqDist(p)\n",
            "vocab = fdist.keys()\n",
            "vocab[:100]\n",
            "\n",
            "What I am doing wrong?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pythonnlpnltk\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "Improve this question\n",
            "\n",
            "\n",
            "\n",
            "                        Follow\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "edited Oct 31, 2022 at 1:33\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Michael Delgado\n",
            "\n",
            "14.4k44 gold badges3232 silver badges5656 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            asked Jan 8, 2011 at 16:12\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "afg102afg102\n",
            "\n",
            "37122 gold badges44 silver badges44 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Please add a link to the example.\n",
            "\n",
            "– Andrew Dalke\n",
            "\n",
            "Jan 8, 2011 at 16:47\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NB, better Python idiom is: with open(fileurl) as file_y: ... or for line in open(file url):\n",
            "\n",
            "– smci\n",
            "\n",
            "Aug 1, 2013 at 8:06\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Add a comment\n",
            " | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                        6 Answers\n",
            "                                    6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            Sorted by:\n",
            "        \n",
            "\n",
            "            Reset to default\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                        Highest score (default)\n",
            "                    \n",
            "\n",
            "                        Trending (recent votes count more)\n",
            "                    \n",
            "\n",
            "                        Date modified (newest first)\n",
            "                    \n",
            "\n",
            "                        Date created (oldest first)\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            51\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FreqDist expects an iterable of tokens. A string is iterable --- the iterator yields every character.\n",
            "Pass your text to a tokenizer first, and pass the tokens to FreqDist.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "Improve this answer\n",
            "\n",
            "\n",
            "\n",
            "                        Follow\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            answered Jan 8, 2011 at 16:44\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Alex BrasetvikAlex Brasetvik\n",
            "\n",
            "11.4k22 gold badges3737 silver badges3737 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Indeed it does, but its docstring doesn't say that anywhere, nor do its error messages, and it would be trivial for its __init__() to either raise an error message saying so on non-iterator input, or accept a sequence and convert it to an iterator.\n",
            "\n",
            "– smci\n",
            "\n",
            "Jul 28, 2013 at 5:23\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "@afg102 If it has worked, please accept the answer so that others also know what is the solution to the problem.\n",
            "\n",
            "– rishi\n",
            "\n",
            "Aug 8, 2013 at 9:44\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Add a comment\n",
            " | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            33\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FreqDist runs on an array of tokens. You're sending it a an array of characters (a string) where you should have tokenized the input first:\n",
            "words = nltk.tokenize.word_tokenize(p)\n",
            "fdist = FreqDist(words)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "Improve this answer\n",
            "\n",
            "\n",
            "\n",
            "                        Follow\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            answered Jun 27, 2011 at 23:58\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Eran KampfEran Kampf\n",
            "\n",
            "8,96688 gold badges4949 silver badges4747 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Add a comment\n",
            " | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            23\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NLTK's FreqDist accepts any iterable. As a string is iterated character by character, it is pulling things apart in the way that you're experiencing.\n",
            "In order to do count words, you need to feed FreqDist words. How do you do that? Well, you might think (as others have suggested in the answer to your question) to feed the whole file to nltk.tokenize.word_tokenize.\n",
            ">>> # first, let's import the dependencies\n",
            ">>> import nltk\n",
            ">>> from nltk.probability import FreqDist\n",
            "\n",
            ">>> # wrong :(\n",
            ">>> words = nltk.tokenize.word_tokenize(p)\n",
            ">>> fdist = FreqDist(words)\n",
            "\n",
            "word_tokenize builds word models from sentences. It needs to be fed each sentence one at a time. It will do a relatively poor job when given whole paragraphs or even documents.\n",
            "So, what to do? Easy, add in a sentence tokenizer!\n",
            ">>> fdist = FreqDist()\n",
            ">>> for sentence in nltk.tokenize.sent_tokenize(p):\n",
            "...     for word in nltk.tokenize.word_tokenize(sentence):\n",
            ">>>         fdist[word] += 1\n",
            "\n",
            "One thing to bear in mind is that there are many ways to tokenize text. The modules nltk.tokenize.sent_tokenize and nltk.tokenize.word_tokenize simply pick a reasonable default for relatively clean, English text. There are several other options to chose from, which you can read about in the API documentation.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "Improve this answer\n",
            "\n",
            "\n",
            "\n",
            "                        Follow\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "edited Oct 3, 2017 at 23:19\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            answered Jan 8, 2011 at 19:52\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tim McNamaraTim McNamara\n",
            "\n",
            "18.1k44 gold badges5252 silver badges8484 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The OP doesn't want letter frequencies! (noone else does either...) They want word frequencies.\n",
            "\n",
            "– smci\n",
            "\n",
            "Jul 28, 2013 at 4:57\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Actually, letter frequencies are very common features for automatic language detection.\n",
            "\n",
            "– Tim McNamara\n",
            "\n",
            "Jul 29, 2013 at 9:56\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "True, for that niche. Also decryption. In general not much though.\n",
            "\n",
            "– smci\n",
            "\n",
            "Aug 1, 2013 at 3:44\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Really useful answer, however it seems to be a bit outdated: AttributeError: 'FreqDist' object has no attribute 'inc'. Not complaining, just throwing it out there for others to be aware of this. I'll try to figure out an answer to this ;) Thanks\n",
            "\n",
            "– Aleksander Lidtke\n",
            "\n",
            "Jun 19, 2016 at 13:08\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Yes, things have changed quite a bit to some of the internal NLTK APIs in the last 5 years! Will update the code :)\n",
            "\n",
            "– Tim McNamara\n",
            "\n",
            "Jun 23, 2016 at 21:54\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " | \n",
            "Show 1 more comment\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            9\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You simply have to use it like this:\n",
            "import nltk\n",
            "from nltk.probability import FreqDist\n",
            "\n",
            "sentence='''This is my sentence'''\n",
            "tokens = nltk.tokenize.word_tokenize(sentence)\n",
            "fdist=FreqDist(tokens)\n",
            "\n",
            "The variable fdist is of the type \"class 'nltk.probability.FreqDist\"  and contains the frequency distribution of words.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "Improve this answer\n",
            "\n",
            "\n",
            "\n",
            "                        Follow\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "edited May 20, 2019 at 15:17\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CommunityBot\n",
            "\n",
            "111 silver badge\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            answered Aug 8, 2013 at 6:37\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Aakash AnujAakash Anuj\n",
            "\n",
            "3,79377 gold badges3535 silver badges4747 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Add a comment\n",
            " | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            1\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Your_string = \"here is my string\"\n",
            "tokens = Your_string.split()\n",
            "\n",
            "Do this way, and then use the NLTK functions\n",
            "it will give your tokens in words but not in characters\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "Improve this answer\n",
            "\n",
            "\n",
            "\n",
            "                        Follow\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            answered Feb 29, 2020 at 9:33\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MusadiqMusadiq\n",
            "\n",
            "2911 silver badge55 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Add a comment\n",
            " | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            0\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text_dist = nltk.FreqDist(word for word in list(text) if word.isalpha())\n",
            "top1_text1 = text_dist.max()\n",
            "maxfreq = top1_text1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Share\n",
            "\n",
            "\n",
            "Improve this answer\n",
            "\n",
            "\n",
            "\n",
            "                        Follow\n",
            "                    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            answered Oct 3, 2020 at 16:06\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pranab BijoypuriPranab Bijoypuri\n",
            "\n",
            "2122 bronze badges\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "While this code may answer the question, it would be better to explain how it solves the problem without introducing others and why to use it. Code-only answers are not useful in the long run.\n",
            "\n",
            "– jnovack\n",
            "\n",
            "Oct 3, 2020 at 16:29\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Add a comment\n",
            " | \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                    Your Answer\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Reminder: Answers generated by artificial intelligence tools are not allowed on Stack Overflow. Learn more\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Thanks for contributing an answer to Stack Overflow!Please be sure to answer the question. Provide details and share your research!But avoid …Asking for help, clarification, or responding to other answers.Making statements based on opinion; back them up with references or personal experience.To learn more, see our tips on writing great answers.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Draft saved\n",
            "Draft discarded\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign up or log in\n",
            "\n",
            "\n",
            " Sign up using Google\n",
            "                        \n",
            "\n",
            " Sign up using Facebook\n",
            "                        \n",
            "\n",
            " Sign up using Email and Password\n",
            "                        \n",
            "\n",
            "\n",
            "\n",
            "Submit\n",
            "\n",
            "Post as a guest\n",
            "\n",
            "\n",
            "Name\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Email\n",
            "Required, but never shown\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Post as a guest\n",
            "\n",
            "\n",
            "Name\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Email\n",
            "Required, but never shown\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                            Post Your Answer\n",
            "                                        \n",
            "\n",
            "                                            Discard\n",
            "                                        \n",
            "\n",
            "                                                By clicking “Post Your Answer”, you agree to our terms of service and acknowledge you have read our privacy policy.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Not the answer you're looking for? Browse other questions tagged pythonnlpnltk or ask your own question.                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NLP\n",
            "Collective\n",
            "\n",
            "Join the discussion\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " This question is in a collective: a subcommunity defined by tags with relevant content and experts.\n",
            "                                        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                            The Overflow Blog\n",
            "                        \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Would you trust an AI bot to find the fix for vulnerabilities in your code?\n",
            "\n",
            "\n",
            "\n",
            "                            Featured on Meta\n",
            "                        \n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Site maintenance - Saturday, February 24th, 2024, 14:00 - 22:00 UTC (9 AM - 5...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Upcoming privacy updates: removal of the Activity data section and Google...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Temporary policy: Generative AI (e.g., ChatGPT) is banned\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "Discussions update: Expansion to all tags\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "Linked\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            "\n",
            "To find frequency of every word in text file in python\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            "\n",
            "SGML Parser in Python\n",
            "\n",
            "\n",
            "\n",
            "-1\n",
            "\n",
            "Most common n words in a text\n",
            "\n",
            "\n",
            "\n",
            "0\n",
            "\n",
            "TypeError: unhashable type: 'list' for nltk.FreqDist even though I've converted my list into a tuple\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Related\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            "\n",
            "Sum up the number of words frequency using FreqDist, python\n",
            "\n",
            "\n",
            "\n",
            "3\n",
            "\n",
            "FreqDist using NLTK\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            "\n",
            "Python Frequency Distribution (FreqDist / NLTK) Issue\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "\n",
            "Perform thresholding in nltk.FreqDist\n",
            "\n",
            "\n",
            "\n",
            "0\n",
            "\n",
            "FreqDist with nltk: ValueError: too many values to unpack\n",
            "\n",
            "\n",
            "\n",
            "4\n",
            "\n",
            "Why FreqDist comparisons in NLTK are not symmetric? i.e. '>' and '<' behave differently\n",
            "\n",
            "\n",
            "\n",
            "0\n",
            "\n",
            "NLTK FreqDist counting two words as one\n",
            "\n",
            "\n",
            "\n",
            "2\n",
            "\n",
            "How to convert the FreqDist into a dictionary?\n",
            "\n",
            "\n",
            "\n",
            "3\n",
            "\n",
            "NLTK FreqDist to a table using pandas\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "\n",
            "Grouping two words as one in FreqDist\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            Hot Network Questions\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                    Did any developing country criticize China's industrial policies, which include dumping?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Applications of High School Geometry\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    How do I read aloud a range of years with a slash?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Markets in Germany with a large selection of seafood\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    In the London, after 1.d4 d5 2.Bf4 c5 3.dxc5 Nc6 4.Nf3, why is \"blocking your bishop\" the best move\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Using MapThread with pure function and variable number of elements\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Why is \"tyrannis\" in \"sic semper tyrannis\" interpreted as \"to tyrants\"?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Why doesn't the Moon disrupt the orbits of geostationary satellites?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    How to put big brackets around minipage\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Odd use(s) of \"bauen\"\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Remove all text files with non-US-ASCII text encoding from current folder on Linux\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    What specifically can’t the Wish spell do?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Novel - Disguised alien Tock Von visits Earth, meets a girl, gets committed to a psychiatric hospital\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Cloning the map layer using PyQGIS\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Enumerate the Phat-fingered-lights-out numbers\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    What exactly is transmission time?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    What spares were taken on Apollo missions, and what was left behind? The question of gloves\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    How can Australia can be a member of the Antarctic Treaty while still making a territorial claim in the region?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Ingenuity extended static mission science possibilites\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    What's the word similar to jittery in spelling or pronunciation, but means some privately-run transportation by small vehicles?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    How does this code work?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Re-heat Sous Vide Roast Beef\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Shimano clipless pedals and cleats\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                    Is it possible for truth to be set by humans?\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "            more hot questions\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            Question feed\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                Subscribe to RSS\n",
            "            \n",
            "\n",
            "\n",
            "\n",
            "                        Question feed\n",
            "                        To subscribe to this RSS feed, copy and paste this URL into your RSS reader.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "lang-py\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stack Overflow\n",
            "\n",
            "Questions\n",
            "Help\n",
            "\n",
            "\n",
            "\n",
            "Products\n",
            "\n",
            "Teams\n",
            "Advertising\n",
            "Collectives\n",
            "Talent\n",
            "\n",
            "\n",
            "\n",
            "Company\n",
            "\n",
            "About\n",
            "Press\n",
            "Work Here\n",
            "Legal\n",
            "Privacy Policy\n",
            "Terms of Service\n",
            "Contact Us\n",
            "Cookie Settings\n",
            "Cookie Policy\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Stack Exchange Network\n",
            "\n",
            "\n",
            "\n",
            "                                    Technology\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "                                    Culture & recreation\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "                                    Life & arts\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "                                    Science\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "                                    Professional\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "                                    Business\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "                                    API\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "                                    Data\n",
            "                                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Blog\n",
            "Facebook\n",
            "Twitter\n",
            "LinkedIn\n",
            "Instagram\n",
            "\n",
            "\n",
            "Site design / logo © 2024 Stack Exchange Inc; user contributions licensed under CC BY-SA.                    rev 2024.2.16.5008\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN **R**"
      ],
      "metadata": {
        "id": "XyLgq8y18YxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tm (Text Mining Infrastructure in R):**\n",
        "\n",
        "The tm package provides a framework for text mining applications within R. It supports various text preprocessing tasks, including text cleaning, stemming, and term-document matrix creation.\n",
        "\n",
        "**quanteda:**\n",
        "\n",
        "quanteda is a comprehensive package for text analysis, providing tools for text corpus creation, document-feature matrix construction, and various text mining and analysis functions.\n",
        "\n",
        "**NLP (Natural Language Processing):**\n",
        "\n",
        "The NLP package provides functions for natural language processing tasks, including tokenization, stemming, and part-of-speech tagging. It is often used in conjunction with other text analysis libraries.\n",
        "\n",
        "**openNLP:**\n",
        "\n",
        "openNLP is an R interface to Apache OpenNLP, which is a library for natural language processing. It provides functions for part-of-speech tagging, named entity recognition, and other language processing tasks.\n",
        "\n",
        "**rvest:**\n",
        "\n",
        "rvest is a powerful package for web scraping in R. It is particularly useful for extracting information from HTML and XML documents on the web."
      ],
      "metadata": {
        "id": "GKI0P5CTJNqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tm\")\n",
        "install.packages(\"rvest\")\n",
        "install.packages(\"tokenizers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DzrsEO58WLY",
        "outputId": "94de288d-37b7-44fc-94c2-875f252237f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tm)\n",
        "library(rvest)\n",
        "library(NLP)\n",
        "library(tokenizers)\n",
        "library(SnowballC)"
      ],
      "metadata": {
        "id": "ok7mQ6BTBf4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text <- \"He raced to the grocery store. He went inside but realized he forgot his wallet. He raced back home to grab it. Once he found it, he raced to the car again and drove back to the grocery store.\"\n",
        "sent_tokens <- unlist(tokenize_sentences(text))\n",
        "word_tokens <- unlist(tokenize_words(text))\n",
        "cat(\"Sentence Tokens:\", sent_tokens, \"\\n\")\n",
        "cat(\"Word Tokens:\", word_tokens, \"\\n\")\n",
        "# Frequency Distribution\n",
        "fdist <- table(unlist(word_tokens))\n",
        "print(head(sort(fdist, decreasing = TRUE), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfS7-FAdBto2",
        "outputId": "8d944b3d-7f17-489d-f540-c29b94e6f28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: He raced to the grocery store. He went inside but realized he forgot his wallet. He raced back home to grab it. Once he found it, he raced to the car again and drove back to the grocery store. \n",
            "Word Tokens: he raced to the grocery store he went inside but realized he forgot his wallet he raced back home to grab it once he found it he raced to the car again and drove back to the grocery store \n",
            "\n",
            "he to \n",
            " 6  4 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords and punctuations\n",
        "stop_words <- stopwords(\"en\")\n",
        "filtered_tokens <- word_tokens[!(word_tokens %in% stop_words) & grepl(\"[a-zA-Z]\", word_tokens)]\n",
        "cat(\"Filtered Tokens (without stopwords and punctuations):\", filtered_tokens, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGAk9Mx7Bu9I",
        "outputId": "69d6070d-bc7a-423f-d84f-0e38796daa4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (without stopwords and punctuations): raced grocery store went inside realized forgot wallet raced back home grab found raced car drove back grocery store \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmed_tokens <- wordStem(filtered_tokens, language = \"en\")\n",
        "\n",
        "# Lemmatization\n",
        "lemmatized_text <- tolower(text)\n",
        "lemmatized_text <- wordStem(lemmatized_text, language = \"en\")\n",
        "cat(\"Stemmed Tokens:\", stemmed_tokens, \"\\n\")\n",
        "cat(\"Lemmatized Text:\", lemmatized_text, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpoKIk5JB9o0",
        "outputId": "3695fdcb-6d90-47bc-f6e7-1392dd4e25f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Tokens: race groceri store went insid realiz forgot wallet race back home grab found race car drove back groceri store \n",
            "Lemmatized Text: he raced to the grocery store. he went inside but realized he forgot his wallet. he raced back home to grab it. once he found it, he raced to the car again and drove back to the grocery store. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Web Scraping\n",
        "url <- 'http://quotes.toscrape.com/'\n",
        "web_page <- read_html(url)\n",
        "web_text <- html_text(web_page)\n",
        "cat(\"Scraped Data from the Website:\\n\", web_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGBUOS8oB-m7",
        "outputId": "6c468e65-584d-42ff-aa16-36ef82b9dffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped Data from the Website:\n",
            " Quotes to Scrape\n",
            "    \n",
            "        \n",
            "            \n",
            "                \n",
            "                    Quotes to Scrape\n",
            "                \n",
            "            \n",
            "            \n",
            "                \n",
            "                \n",
            "                    Login\n",
            "                \n",
            "                \n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "\n",
            "    \n",
            "\n",
            "    \n",
            "        “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
            "        by Albert Einstein\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            change\n",
            "            \n",
            "            deep-thoughts\n",
            "            \n",
            "            thinking\n",
            "            \n",
            "            world\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
            "        by J.K. Rowling\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            abilities\n",
            "            \n",
            "            choices\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
            "        by Albert Einstein\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            inspirational\n",
            "            \n",
            "            life\n",
            "            \n",
            "            live\n",
            "            \n",
            "            miracle\n",
            "            \n",
            "            miracles\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
            "        by Jane Austen\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            aliteracy\n",
            "            \n",
            "            books\n",
            "            \n",
            "            classic\n",
            "            \n",
            "            humor\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
            "        by Marilyn Monroe\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            be-yourself\n",
            "            \n",
            "            inspirational\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “Try not to become a man of success. Rather become a man of value.”\n",
            "        by Albert Einstein\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            adulthood\n",
            "            \n",
            "            success\n",
            "            \n",
            "            value\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “It is better to be hated for what you are than to be loved for what you are not.”\n",
            "        by André Gide\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            life\n",
            "            \n",
            "            love\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “I have not failed. I've just found 10,000 ways that won't work.”\n",
            "        by Thomas A. Edison\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            edison\n",
            "            \n",
            "            failure\n",
            "            \n",
            "            inspirational\n",
            "            \n",
            "            paraphrased\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
            "        by Eleanor Roosevelt\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            misattributed-eleanor-roosevelt\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "        “A day without sunshine is like, you know, night.”\n",
            "        by Steve Martin\n",
            "        (about)\n",
            "        \n",
            "        \n",
            "            Tags:\n",
            "            humor\n",
            "            \n",
            "            obvious\n",
            "            \n",
            "            simile\n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "    \n",
            "                Next →\n",
            "            \n",
            "            \n",
            "        \n",
            "    \n",
            "        \n",
            "            Top Ten tags\n",
            "            \n",
            "            \n",
            "            love\n",
            "            \n",
            "            \n",
            "            \n",
            "            inspirational\n",
            "            \n",
            "            \n",
            "            \n",
            "            life\n",
            "            \n",
            "            \n",
            "            \n",
            "            humor\n",
            "            \n",
            "            \n",
            "            \n",
            "            books\n",
            "            \n",
            "            \n",
            "            \n",
            "            reading\n",
            "            \n",
            "            \n",
            "            \n",
            "            friendship\n",
            "            \n",
            "            \n",
            "            \n",
            "            friends\n",
            "            \n",
            "            \n",
            "            \n",
            "            truth\n",
            "            \n",
            "            \n",
            "            \n",
            "            simile\n",
            "            \n",
            "            \n",
            "        \n",
            "    \n",
            "\n",
            "\n",
            "    \n",
            "    \n",
            "            \n",
            "                Quotes by: GoodReads.com\n",
            "            \n",
            "            \n",
            "                Made with ❤ by Zyte\n",
            "            \n",
            "        \n",
            "    "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iijXXvTCCah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}